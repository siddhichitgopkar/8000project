finetuned = adding another layer to make it to specific tasks
long contect -> extension of tokens from 8k to 128k
5 shot: algrbra question where you give examples of questions and answers so the model will know the task, what output should be, and the llm will learn; 5 shot = 5 demonstrations
other perspectives for trustworthiness-> machine integrity, hallucination (model generates something thats not real like asking who is president in 2025 and itll just generate a random answer); 
avoid hallucination by providing it with knowledge - give it long paragraph or incremental learning to expand training data but thats costly 

